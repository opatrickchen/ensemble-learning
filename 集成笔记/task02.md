**【Task02】**

    学习内容：回归问题（2天）  
    打卡截止时间⏰：07月16日03:00 
    学习形式：视频+教案
        1.学习CH2 机器学习基础的对应内容，如图片左半部分黑体；
        2.掌握机器学习基本分类，并使用sklearn进行回归应用；
        3.完成对应的作业，如图片右半部分；
        4.整理学习笔记，输出链接在小程序中打卡。

-------

**【学习资料】**

    视频链接：https://www.bilibili.com/video/BV1Mb4y1o7ck?from=search&seid=6085778383215596866
    教案链接：https://github.com/datawhalechina/ensemble-learning
    PS: 由于CH2文档较长，大家根据图片以及视频导引，掌握对应知识点。

-------

**【学习笔记】**

    Bivariate function
    Multivariate function

    Partial derivative
    Gradient: 梯度的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）  
    Jacobian Matrix (m->n): 梯度向量是雅克比矩阵的特例！(m=1)  
    Hessian Matrix: Hessian矩阵是梯度向量g(x)对自变量x的Jacobian矩阵  
